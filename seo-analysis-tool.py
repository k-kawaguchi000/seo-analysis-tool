import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import json
import time
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from urllib.parse import urlparse, urljoin
import ssl

# SSLè¨¼æ˜æ›¸ã®æ¤œè¨¼ã‚’ãƒã‚¤ãƒ‘ã‚¹ï¼ˆå®‰å…¨ã§ãªã„ã‚µã‚¤ãƒˆã‚‚ã‚¯ãƒ­ãƒ¼ãƒ«ã§ãã‚‹ã‚ˆã†ã«ï¼‰
ssl._create_default_https_context = ssl._create_unverified_context

# NLTKã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

# Streamlitã®è¨­å®š
st.set_page_config(
    page_title="SEOåˆ†æãƒ»æ”¹å–„è‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
st.markdown("""
<style>
    .main-header {font-size: 2.5rem; font-weight: 700; color: #1E3A8A; margin-bottom: 1rem;}
    .sub-header {font-size: 1.8rem; font-weight: 600; color: #2563EB; margin-top: 2rem; margin-bottom: 1rem;}
    .section-header {font-size: 1.5rem; font-weight: 600; color: #3B82F6; margin-top: 1.5rem; margin-bottom: 0.75rem;}
    .highlight {background-color: #DBEAFE; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0;}
    .score-card {background-color: #F3F4F6; padding: 1.5rem; border-radius: 0.5rem; box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1); margin-bottom: 1rem;}
    .metric-good {color: #059669; font-weight: 600;}
    .metric-warning {color: #D97706; font-weight: 600;}
    .metric-bad {color: #DC2626; font-weight: 600;}
    .recommendation {background-color: #ECFDF5; padding: 0.75rem; border-left: 4px solid #10B981; margin: 0.75rem 0;}
    .stProgress > div > div > div > div {background-color: #3B82F6;}
</style>
""", unsafe_allow_html=True)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã—ãŸWebãƒšãƒ¼ã‚¸ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°æ©Ÿèƒ½
@st.cache_data(ttl=3600)
def crawl_website(url, max_pages=10):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹é–¢æ•°
    max_pages: ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
    """
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            
        visited_urls = set()
        to_visit = [url]
        pages_data = []
        
        # User-Agentè¨­å®š
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        base_domain = urlparse(url).netloc
        
        while to_visit and len(visited_urls) < max_pages:
            current_url = to_visit.pop(0)
            
            if current_url in visited_urls:
                continue
                
            try:
                response = requests.get(current_url, headers=headers, timeout=10)
                visited_urls.add(current_url)
                
                if response.status_code != 200:
                    continue
                    
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿åé›†
                title = soup.title.string.strip() if soup.title else "No Title"
                
                # H1ã‚¿ã‚°
                h1_tag = soup.find('h1')
                h1_text = h1_tag.get_text().strip() if h1_tag else "No H1"
                
                # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³
                meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
                meta_desc = meta_desc_tag['content'].strip() if meta_desc_tag and 'content' in meta_desc_tag.attrs else "No Meta Description"
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                meta_keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
                meta_keywords = meta_keywords_tag['content'].strip() if meta_keywords_tag and 'content' in meta_keywords_tag.attrs else ""
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ†ã‚­ã‚¹ãƒˆ
                body_text = soup.body.get_text(" ", strip=True) if soup.body else ""
                word_count = len(body_text.split())
                
                # ç”»åƒæ•°ã¨altå±æ€§
                images = soup.find_all('img')
                img_count = len(images)
                img_with_alt = sum(1 for img in images if img.get('alt'))
                
                # H2, H3ã‚¿ã‚°ã®æ•°
                h2_count = len(soup.find_all('h2'))
                h3_count = len(soup.find_all('h3'))
                
                # å†…éƒ¨ãƒªãƒ³ã‚¯åé›†
                internal_links = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if not href.startswith(('http://', 'https://')):
                        href = urljoin(current_url, href)
                    
                    # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®ãƒªãƒ³ã‚¯ã®ã¿è¿½åŠ 
                    if urlparse(href).netloc == base_domain:
                        internal_links.append(href)
                        if href not in visited_urls and href not in to_visit:
                            to_visit.append(href)
                
                # ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                page_data = {
                    'url': current_url,
                    'title': title,
                    'h1': h1_text,
                    'meta_description': meta_desc,
                    'meta_keywords': meta_keywords,
                    'word_count': word_count,
                    'image_count': img_count,
                    'images_with_alt': img_with_alt,
                    'h2_count': h2_count,
                    'h3_count': h3_count,
                    'internal_links_count': len(set(internal_links)),
                    'internal_links': list(set(internal_links))
                }
                
                pages_data.append(page_data)
                
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®é€Ÿåº¦åˆ¶é™ï¼ˆã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ãŸã‚ï¼‰
                time.sleep(1)
                
            except Exception as e:
                print(f"Error crawling {current_url}: {e}")
                continue
        
        return pages_data
    
    except Exception as e:
        print(f"Error in crawl_website: {e}")
        return []

# ãƒšãƒ¼ã‚¸ã‚¹ãƒ”ãƒ¼ãƒ‰è¨ˆç®—é–¢æ•°
def calculate_page_speed_score(word_count, img_count):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªãƒšãƒ¼ã‚¸ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå®Ÿéš›ã®ãƒ„ãƒ¼ãƒ«ã§ã¯PageSpeed Insights APIã‚’ä½¿ç”¨ï¼‰
    """
    base_score = 100
    
    # æ–‡å­—æ•°ãŒå¤šã™ãã‚‹å ´åˆã¯æ¸›ç‚¹
    if word_count > 3000:
        base_score -= min(20, (word_count - 3000) // 500)
        
    # ç”»åƒãŒå¤šã™ãã‚‹å ´åˆã¯æ¸›ç‚¹
    if img_count > 10:
        base_score -= min(15, (img_count - 10) * 2)
    
    # ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ ã‚’åŠ ãˆã‚‹ï¼ˆå®Ÿéš›ã®APIã¨åŒæ§˜ã«ãƒãƒ©ã¤ãã‚’æŒãŸã›ã‚‹ï¼‰
    base_score += np.random.randint(-5, 6)
    
    return max(0, min(100, base_score))

# SEOã‚¹ã‚³ã‚¢è¨ˆç®—é–¢æ•°
def calculate_seo_scores(pages_data):
    """
    å„ãƒšãƒ¼ã‚¸ã®SEOã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    """
    seo_scores = {
        "content": [],
        "internal": [],
        "external": [],
        "total": []
    }
    
    for page in pages_data:
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ã‚³ã‚¢è¨ˆç®—
        content_score = 0
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®è©•ä¾¡
        if page['title'] and len(page['title']) > 10 and len(page['title']) < 70:
            content_score += 20
        elif page['title']:
            content_score += 10
            
        # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®è©•ä¾¡
        if page['meta_description'] and 70 <= len(page['meta_description']) <= 160:
            content_score += 20
        elif page['meta_description']:
            content_score += 10
            
        # H1ã‚¿ã‚°ã®è©•ä¾¡
        if page['h1'] and page['h1'] != "No H1":
            content_score += 15
            
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ã®è©•ä¾¡
        if page['word_count'] >= 1500:
            content_score += 20
        elif page['word_count'] >= 800:
            content_score += 15
        elif page['word_count'] >= 400:
            content_score += 10
        else:
            content_score += 5
            
        # è¦‹å‡ºã—æ§‹é€ ã®è©•ä¾¡
        if page['h2_count'] > 0 and page['h3_count'] > 0:
            content_score += 15
        elif page['h2_count'] > 0:
            content_score += 10
            
        # ç”»åƒã®altå±æ€§è©•ä¾¡
        if page['image_count'] > 0 and page['images_with_alt'] / page['image_count'] >= 0.8:
            content_score += 10
        elif page['image_count'] > 0 and page['images_with_alt'] > 0:
            content_score += 5
        
        # å†…éƒ¨SEOã‚¹ã‚³ã‚¢è¨ˆç®—
        internal_score = 0
        
        # å†…éƒ¨ãƒªãƒ³ã‚¯æ•°ã®è©•ä¾¡
        if page['internal_links_count'] >= 10:
            internal_score += 30
        elif page['internal_links_count'] >= 5:
            internal_score += 20
        elif page['internal_links_count'] > 0:
            internal_score += 10
            
        # ãƒšãƒ¼ã‚¸ã‚¹ãƒ”ãƒ¼ãƒ‰ã®è©•ä¾¡ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        page_speed = calculate_page_speed_score(page['word_count'], page['image_count'])
        if page_speed >= 90:
            internal_score += 30
        elif page_speed >= 70:
            internal_score += 20
        elif page_speed >= 50:
            internal_score += 10
        
        # URLã®è©•ä¾¡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã§èª­ã¿ã‚„ã™ã„ã‹ï¼‰
        url_path = urlparse(page['url']).path
        if len(url_path.split('/')) <= 3 and not re.search(r'\d{10,}', url_path):
            internal_score += 20
        else:
            internal_score += 10
            
        # SSLå¯¾å¿œè©•ä¾¡
        if page['url'].startswith('https://'):
            internal_score += 20
            
        # å¤–éƒ¨SEOã‚¹ã‚³ã‚¢ï¼ˆAPIãŒãªã„ãŸã‚ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ï¼‰
        external_score = np.random.randint(60, 90)
        
        # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        total_score = int(content_score * 0.4 + internal_score * 0.3 + external_score * 0.3)
        
        # ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜
        seo_scores["content"].append(content_score)
        seo_scores["internal"].append(internal_score)
        seo_scores["external"].append(external_score)
        seo_scores["total"].append(total_score)
    
    return seo_scores

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æé–¢æ•°
def analyze_keywords(pages_data, keywords):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾é »åº¦ã¨é–¢é€£æ€§ã‚’åˆ†æã™ã‚‹é–¢æ•°
    """
    keyword_analysis = {}
    
    for keyword in keywords:
        keyword_lower = keyword.lower()
        matches = []
        
        for page in pages_data:
            content = f"{page['title']} {page['meta_description']} {page['h1']}".lower()
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°
            count = content.count(keyword_lower)
            
            # ã‚ˆã‚Šè©³ç´°ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æï¼ˆå®Ÿéš›ã«ã¯ã‚‚ã£ã¨è¤‡é›‘ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
            if count > 0:
                matches.append({
                    'url': page['url'],
                    'title': page['title'],
                    'count': count,
                    'density': round(count / max(1, len(content.split())) * 100, 2)
                })
        
        # æ¤œç´¢é †ä½ã¨æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
        search_volume = np.random.randint(100, 10000)
        current_rank = np.random.randint(1, 100)
        
        # 30æ—¥é–“ã®æ¨ç§»ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30, 0, -1)]
        base_rank = current_rank + np.random.randint(0, 20)  # æœ€åˆã®é †ä½ã¯ç¾åœ¨ã‚ˆã‚Šå°‘ã—æ‚ªã„
        rankings = []
        
        for i in range(len(dates)):
            # å¾ã€…ã«æ”¹å–„ã—ã¦ã„ãå‚¾å‘ï¼ˆãƒ©ãƒ³ãƒ€ãƒ è¦ç´ ã‚ã‚Šï¼‰
            improvement = (i / len(dates)) * np.random.randint(5, 20)
            rank = max(1, int(base_rank - improvement + np.random.randint(-3, 4)))
            rankings.append(rank)
        
        # æœ€æ–°ã®é †ä½ãŒç¾åœ¨ã®é †ä½ã¨ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«èª¿æ•´
        rankings[-1] = current_rank
        
        keyword_analysis[keyword] = {
            'matches': matches,
            'search_volume': search_volume,
            'current_rank': current_rank,
            'rankings': rankings,
            'dates': dates,
            'difficulty': np.random.randint(20, 80)
        }
    
    return keyword_analysis

# æ”¹å–„ææ¡ˆç”Ÿæˆé–¢æ•°
def generate_improvements(pages_data, keyword_analysis):
    """
    åˆ†æçµæœã«åŸºã¥ã„ã¦æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    improvements = {
        "content": [],
        "internal": [],
        "external": [],
        "technical": []
    }
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ”¹å–„ææ¡ˆ
    content_issues = []
    
    # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®å•é¡Œãƒã‚§ãƒƒã‚¯
    meta_desc_issues = [page for page in pages_data if not page['meta_description'] or 
                        page['meta_description'] == "No Meta Description" or 
                        len(page['meta_description']) < 70 or 
                        len(page['meta_description']) > 160]
    
    if meta_desc_issues:
        if len(meta_desc_issues) > 1:
            improvements["content"].append(f"{len(meta_desc_issues)}ãƒšãƒ¼ã‚¸ã§ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚é©åˆ‡ãªé•·ã•ï¼ˆ70ã€œ160æ–‡å­—ï¼‰ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        else:
            improvements["content"].append(f"ã€Œ{meta_desc_issues[0]['title']}ã€ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚é©åˆ‡ãªé•·ã•ï¼ˆ70ã€œ160æ–‡å­—ï¼‰ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    
    # H1ã‚¿ã‚°ã®å•é¡Œãƒã‚§ãƒƒã‚¯
    h1_issues = [page for page in pages_data if not page['h1'] or page['h1'] == "No H1"]
    if h1_issues:
        if len(h1_issues) > 1:
            improvements["content"].append(f"{len(h1_issues)}ãƒšãƒ¼ã‚¸ã§H1ã‚¿ã‚°ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å„ãƒšãƒ¼ã‚¸ã«å›ºæœ‰ã®H1ã‚¿ã‚°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        else:
            improvements["content"].append(f"ã€Œ{h1_issues[0]['title']}ã€ãƒšãƒ¼ã‚¸ã«H1ã‚¿ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚é©åˆ‡ãªH1ã‚¿ã‚°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ã®å•é¡Œãƒã‚§ãƒƒã‚¯
    low_content_pages = [page for page in pages_data if page['word_count'] < 600]
    if low_content_pages:
        if len(low_content_pages) > 1:
            improvements["content"].append(f"{len(low_content_pages)}ãƒšãƒ¼ã‚¸ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ä¸»è¦ãƒšãƒ¼ã‚¸ã§ã¯æœ€ä½1,000æ–‡å­—ä»¥ä¸Šã‚’ç›®æŒ‡ã—ã¾ã—ã‚‡ã†ã€‚")
        else:
            improvements["content"].append(f"ã€Œ{low_content_pages[0]['title']}ã€ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚‚ã£ã¨è©³ç´°ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    
    # è¦‹å‡ºã—æ§‹é€ ã®å•é¡Œãƒã‚§ãƒƒã‚¯
    heading_issues = [page for page in pages_data if page['h2_count'] == 0]
    if heading_issues:
        if len(heading_issues) > 1:
            improvements["content"].append(f"{len(heading_issues)}ãƒšãƒ¼ã‚¸ã§è¦‹å‡ºã—æ§‹é€ ï¼ˆH2ã‚¿ã‚°ï¼‰ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´ç†ã—ã€é©åˆ‡ãªè¦‹å‡ºã—æ§‹é€ ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        else:
            improvements["content"].append(f"ã€Œ{heading_issues[0]['title']}ã€ãƒšãƒ¼ã‚¸ã§è¦‹å‡ºã—æ§‹é€ ï¼ˆH2ã‚¿ã‚°ï¼‰ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´ç†ã—ã€é©åˆ‡ãªè¦‹å‡ºã—æ§‹é€ ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
    
    # ç”»åƒã®altå±æ€§ã®å•é¡Œãƒã‚§ãƒƒã‚¯
    alt_issues = [page for page in pages_data if page['image_count'] > 0 and page['images_with_alt'] / page['image_count'] < 0.8]
    if alt_issues:
        improvements["content"].append("è¤‡æ•°ã®ç”»åƒã«altå±æ€§ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã™ã¹ã¦ã®ç”»åƒã«é©åˆ‡ãªä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ´»ç”¨ã®ææ¡ˆ
    if keyword_analysis:
        keyword_pages = sum([len(data['matches']) for data in keyword_analysis.values()])
        if keyword_pages < len(pages_data) * len(keyword_analysis) * 0.5:
            improvements["content"].append("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ´»ç”¨ãŒä¸ååˆ†ã§ã™ã€‚ã‚ˆã‚Šå¤šãã®ãƒšãƒ¼ã‚¸ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªç„¶ã«å–ã‚Šå…¥ã‚Œã¦ãã ã•ã„ã€‚")
    
    # å†…éƒ¨SEOæ”¹å–„ææ¡ˆ
    
    # å†…éƒ¨ãƒªãƒ³ã‚¯ã®å•é¡Œãƒã‚§ãƒƒã‚¯
    low_link_pages = [page for page in pages_data if page['internal_links_count'] < 5]
    if low_link_pages:
        if len(low_link_pages) > 1:
            improvements["internal"].append(f"{len(low_link_pages)}ãƒšãƒ¼ã‚¸ã§å†…éƒ¨ãƒªãƒ³ã‚¯ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã®ãƒªãƒ³ã‚¯ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚")
        else:
            improvements["internal"].append(f"ã€Œ{low_link_pages[0]['title']}ã€ãƒšãƒ¼ã‚¸ã®å†…éƒ¨ãƒªãƒ³ã‚¯ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã®ãƒªãƒ³ã‚¯ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚")
    
    # URLã®æœ€é©åŒ–
    complex_urls = [page for page in pages_data if len(urlparse(page['url']).path.split('/')) > 3 or re.search(r'\d{10,}', urlparse(page['url']).path)]
    if complex_urls:
        improvements["internal"].append("è¤‡é›‘ãªURLæ§‹é€ ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚URLã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ§‹é€ ã«ã—ã¦ãã ã•ã„ã€‚")
    
    # HTTPSã®ãƒã‚§ãƒƒã‚¯
    non_https = [page for page in pages_data if not page['url'].startswith('https://')]
    if non_https:
        improvements["internal"].append("HTTPSãŒå°å…¥ã•ã‚Œã¦ã„ãªã„ãƒšãƒ¼ã‚¸ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨SEOã®ãŸã‚ã«ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‚’HTTPSã«ç§»è¡Œã—ã¦ãã ã•ã„ã€‚")
    
    # ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œã®ææ¡ˆï¼ˆå®Ÿéš›ã«ã¯è©³ç´°ãªåˆ†æãŒå¿…è¦ï¼‰
    improvements["internal"].append("ãƒ¢ãƒã‚¤ãƒ«ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚Googleã®ãƒ¢ãƒã‚¤ãƒ«ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æœ€é©åŒ–ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚")
    
    # ãƒšãƒ¼ã‚¸é€Ÿåº¦ã®æœ€é©åŒ–ææ¡ˆ
    improvements["internal"].append("ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿é€Ÿåº¦ã®æœ€é©åŒ–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚ç”»åƒã®åœ§ç¸®ã€JavaScriptã®é…å»¶èª­ã¿è¾¼ã¿ã€ä¸è¦ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®å‰Šé™¤ãªã©ãŒåŠ¹æœçš„ã§ã™ã€‚")
    
    # å¤–éƒ¨SEOæ”¹å–„ææ¡ˆï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šè©³ç´°ãªåˆ†æãŒå¿…è¦ï¼‰
    improvements["external"].append("é«˜å“è³ªãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰ã®ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ç²å¾—ã™ã‚‹ãŸã‚ã«ã€æ¥­ç•Œé–¢é€£ã®ä¿¡é ¼æ€§ã®é«˜ã„ã‚µã‚¤ãƒˆã¨ã®é–¢ä¿‚æ§‹ç¯‰ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
    improvements["external"].append("ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã®ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚åŒã˜ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®éå‰°ãªä½¿ç”¨ã¯é¿ã‘ã‚‹ã¹ãã§ã™ã€‚")
    improvements["external"].append("ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã®å­˜åœ¨æ„Ÿã‚’é«˜ã‚ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚·ã‚§ã‚¢ã‚’ä¿ƒé€²ã—ã¦ãã ã•ã„ã€‚ã‚¤ãƒ³ãƒ•ã‚©ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãªã©å…±æœ‰ã•ã‚Œã‚„ã™ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä½œæˆãŒåŠ¹æœçš„ã§ã™ã€‚")
    
    # æŠ€è¡“çš„SEOæ”¹å–„ææ¡ˆ
    improvements["technical"].append("XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’æœ€æ–°ã®çŠ¶æ…‹ã«ä¿ã¡ã€Google Search Consoleã«å®šæœŸçš„ã«é€ä¿¡ã—ã¦ãã ã•ã„ã€‚")
    improvements["technical"].append("robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€é©åŒ–ã—ã€ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ãŒé©åˆ‡ã«ã‚µã‚¤ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚")
    improvements["technical"].append("é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å•é¡Œã‚’ç¢ºèªã—ã€canonical URLã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    improvements["technical"].append("æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆSchema.orgï¼‰ã‚’å®Ÿè£…ã—ã¦ã€æ¤œç´¢çµæœã§ã®è¡¨ç¤ºã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚")
    improvements["technical"].append("404ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‚’ç¢ºèªã—ã€ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å¾©å…ƒã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
    
    return improvements

# ç«¶åˆåˆ†æé–¢æ•°
def analyze_competitors(competitor_urls, keywords):
    """
    ç«¶åˆã‚µã‚¤ãƒˆã®åŸºæœ¬çš„ãªåˆ†æã‚’è¡Œã†é–¢æ•°ï¼ˆå®Ÿéš›ã«ã¯è©³ç´°ãªAPIãŒå¿…è¦ï¼‰
    """
    competitor_data = {}
    
    for url in competitor_urls:
        # ç«¶åˆã‚µã‚¤ãƒˆã®åŸºæœ¬æƒ…å ±ï¼ˆå®Ÿéš›ã®APIã‚’ä½¿ç”¨ï¼‰
        competitor_data[url] = {
            "seo_score": np.random.randint(40, 95),
            "backlinks": np.random.randint(30, 1000),
            "keywords_ranking": np.random.randint(10, 500),
            "content_score": np.random.randint(50, 95),
            "technical_score": np.random.randint(40, 95),
            "page_speed": np.random.randint(50, 95),
            "domain_authority": np.random.randint(20, 80),
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        keyword_ranks = {}
        for keyword in keywords:
            keyword_ranks[keyword] = np.random.randint(1, 100)
        
        competitor_data[url]["keyword_ranks"] = keyword_ranks
    
    return competitor_data

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆå…¥åŠ›éƒ¨åˆ†ï¼‰
st.sidebar.markdown('<div style="text-align: center;"><h2>SEOåˆ†æãƒ„ãƒ¼ãƒ«è¨­å®š</h2></div>', unsafe_allow_html=True)

# 1. Webã‚µã‚¤ãƒˆURL
website_url = st.sidebar.text_input("Webã‚µã‚¤ãƒˆ URL", "https://example.com")

# 2. ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³è¡¨ç¤ºï¼‰
show_report_urls = st.sidebar.checkbox("ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿URLã‚’è¨­å®šã™ã‚‹")
if show_report_urls:
    gsc_url = st.sidebar.text_input("Google Search Console URL", "")
    ga4_url = st.sidebar.text_input("Google Analytics 4 URL", "")
    ahrefs_url = st.sidebar.text_input("Ahrefs URL", "")
    semrush_url = st.sidebar.text_input("SEMrush URL", "")

# 3. ãƒ‹ãƒ¼ã‚ºï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰
st.sidebar.markdown("### åˆ†æãƒ‹ãƒ¼ã‚ºï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰")
need_new_orders = st.sidebar.checkbox("æ–°è¦å—æ³¨å‘ã‘SEOæ”¹å–„")
need_recruitment = st.sidebar.checkbox("æ±‚äººç²å¾—å‘ã‘SEOå¯¾ç­–")
need_partners = st.sidebar.checkbox("å”åŠ›ä¼šç¤¾å‹Ÿé›†å‘ã‘SEOå¼·åŒ–")
need_competitor = st.sidebar.checkbox("ç«¶åˆã‚µã‚¤ãƒˆã¨ã®æ¯”è¼ƒåˆ†æ")
need_technical = st.sidebar.checkbox("ã‚µã‚¤ãƒˆå…¨ä½“ã®æŠ€è¡“çš„SEOæ”¹å–„")

# 4. èª¿æŸ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
st.sidebar.markdown("### èª¿æŸ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰")
keywords = st.sidebar.text_area("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "SEOå¯¾ç­–, ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°, å†…éƒ¨ãƒªãƒ³ã‚¯æœ€é©åŒ–")
keyword_list = [kw.strip() for kw in keywords.split(",") if kw.strip()]

# 5. ç«¶åˆã‚µã‚¤ãƒˆURLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
show_competitors = st.sidebar.checkbox("ç«¶åˆã‚µã‚¤ãƒˆã‚’è¨­å®šã™ã‚‹")
competitor_urls = []
if show_competitors:
    competitors = st.sidebar.text_area("ç«¶åˆã‚µã‚¤ãƒˆURLï¼ˆ1è¡Œã«1ã¤ï¼‰", "https://competitor1.com\nhttps://competitor2.com")
    competitor_urls = [url.strip() for url in competitors.split("\n") if url.strip()]

# ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã®è¨­å®š
max_pages = st.sidebar.slider("ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°", min_value=3, max_value=50, value=10)

# å®Ÿè¡Œãƒœã‚¿ãƒ³
analyze_button = st.sidebar.button("åˆ†æã‚’å®Ÿè¡Œ", type="primary")

# ãƒ¡ã‚¤ãƒ³ç”»é¢
st.markdown('<div class="main-header">SEOåˆ†æãƒ»æ”¹å–„è‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«</div>', unsafe_allow_html=True)
st.markdown("Webã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æã€å†…éƒ¨å¯¾ç­–ã€å¤–éƒ¨å¯¾ç­–ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æˆ¦ç•¥ã‚’è©•ä¾¡ã—ã€æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

# ã‚¿ãƒ–ã®è¨­å®š
tabs = st.tabs(["ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ", "å†…éƒ¨SEOåˆ†æ", "å¤–éƒ¨SEOåˆ†æ", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ", "æ”¹å–„ææ¡ˆ"])
dashboard_tab, content_tab, internal_tab, external_tab, keyword_tab, recommendations_tab = tabs

# åˆ†æå®Ÿè¡Œæ™‚ã®å‡¦ç†
if analyze_button:
    if website_url == "" or website_url == "https://example.com":
        st.error("æœ‰åŠ¹ãªWebã‚µã‚¤ãƒˆURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner('ã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦åˆ†æã—ã¦ã„ã¾ã™...'):
            # ã‚µã‚¤ãƒˆã®ã‚¯ãƒ­ãƒ¼ãƒ«
            try:
                pages_data = crawl_website(website_url, max_pages=max_pages)
                
                if not pages_data:
                    st.error("ã‚µã‚¤ãƒˆã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸã€‚URLãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                else:
                    # SEOã‚¹ã‚³ã‚¢ã®è¨ˆç®—
                    seo_scores = calculate_seo_scores(pages_data)
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
                    keyword_analysis = analyze_keywords(pages_data, keyword_list)
                    
                    # æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
                    improvements = generate_improvements(pages_data, keyword_analysis)
                    
                    # ç«¶åˆåˆ†æï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                    competitor_data = {}
                    if competitor_urls:
                        competitor_data = analyze_competitors(competitor_urls, keyword_list)
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    st.session_state.pages_data = pages_data
                    st.session_state.seo_scores = seo_scores
                    st.session_state.keyword_analysis = keyword_analysis
                    st.session_state.improvements = improvements
                    st.session_state.competitor_data = competitor_data
                    st.session_state.analyzed = True
                    
                    st.success(f'{len(pages_data)}ãƒšãƒ¼ã‚¸ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼å„ã‚¿ãƒ–ã§è©³ç´°ã‚’ç¢ºèªã§ãã¾ã™ã€‚')
            except Exception as e:
                st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

# åˆ†ææ¸ˆã¿ã§ãªã„å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
if not hasattr(st.session_state, 'analyzed'):
    st.info('ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å¿…è¦æƒ…å ±ã‚’å…¥åŠ›ã—ã€ã€Œåˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚')
    st.session_state.analyzed = False

# åˆ†ææ¸ˆã¿ã®å ´åˆã®å„ã‚¿ãƒ–ã®è¡¨ç¤º
if hasattr(st.session_state, 'analyzed') and st.session_state.analyzed:
    pages_data = st.session_state.pages_data
    seo_scores = st.session_state.seo_scores
    keyword_analysis = st.session_state.keyword_analysis
    improvements = st.session_state.improvements
    competitor_data = st.session_state.competitor_data
    
    # 1. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚¿ãƒ–
    with dashboard_tab:
        st.markdown('<div class="sub-header">SEOç·åˆè©•ä¾¡</div>', unsafe_allow_html=True)
        
        # å…¨ä½“ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã¨è¡¨ç¤º
        overall_score = np.mean(seo_scores["total"])
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric